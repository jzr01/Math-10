{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e251f81",
   "metadata": {},
   "source": [
    "# Homework 5\n",
    "\n",
    "You may submit this homework in a group of 1-3 total students.\n",
    "\n",
    "**Due date**.  Before lecture on Friday of Week 5, October 29th.\n",
    "\n",
    "Name(s): Zhengran Ji\n",
    "\n",
    "UCI ID(s): 87537895"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ececb3",
   "metadata": {},
   "source": [
    "## Setup for multi-variable linear regression\n",
    "For $i = 1, 2, \\ldots, m$, we have an input $\\vec{x}^{\\,(i)} \\in \\mathbb{R}^{n}$ and an output $y^{(i)} \\in \\mathbb{R}$.  \n",
    "\n",
    "Given elements $\\theta_0, \\theta_1, \\ldots, \\theta_n \\in \\mathbb{R}$, we get a corresponding map $\\mathbb{R}^n \\rightarrow \\mathbb{R}$ given by \n",
    "\n",
    "$$\n",
    "(x_1, x_2, \\ldots, x_n) \\mapsto \\theta_0 + \\theta_1 x_1 + \\cdots  + \\theta_n x_n.\n",
    "$$\n",
    "\n",
    "We want to find the values of $\\theta_i$ which best model $\\vec{x}^{\\,(i)} \\leadsto y^{(i)}$.\n",
    "\n",
    "By \"best\", we mean the residual sum of squares loss function is as small as possible, where the loss function is given by \n",
    "\n",
    "$$\n",
    "J = \\frac{1}{m} \\sum_{i = 1}^m \\bigg(y^{(i)} - (\\theta_0 + \\theta_1 x_1^{(i)} + \\cdots  + \\theta_n x_n^{(i)})\\bigg)^2.\n",
    "$$\n",
    "\n",
    "Define an $m \\times (n+1)$ matrix $X$ whose $i$-th row is $(1,  x_1^{(i)}, \\ldots,  x_n^{(i)})$.  Let $\\vec{y}$ denote the $m \\times 1$ column vector $(y^{(1)}, \\ldots, y^{(m)})^T$, and let $\\vec{\\theta}$ denote the $(n+1) \\times 1$ column vector $(\\theta_0, \\theta_1, \\ldots, \\theta_n)^T$.\n",
    "\n",
    "We saw on Friday that any critical point (i.e., point where the gradient is zero) would have to satisfy\n",
    "\n",
    "$$\n",
    "-2\\vec{y}^{\\,T} X  + 2 \\vec{\\theta}^{\\,T}X^T X = (0,\\ldots,0),\n",
    "$$ \n",
    "\n",
    "which after taking transposes would imply\n",
    "\n",
    "$$\n",
    "X^T X \\vec{\\theta} = X^T \\vec{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4d74dd",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004cf924",
   "metadata": {},
   "source": [
    "<p style=\"font-size:20px; color:blue; font-weight:bold\">Question 1:</p>\n",
    "\n",
    "(a) Geometrically, what is the meaning of $y^{(i)} - (\\theta_0 + \\theta_1 x_1^{(i)} + \\cdots  + \\theta_n x_n^{(i)})$?  Use the word *vertical* in your answer.\n",
    "\n",
    "(b) Why do we have the $1/m$ coefficient?\n",
    "\n",
    "(c) Would it make sense to try to maximize the loss function?  Do you think there is a maximum?\n",
    "\n",
    "(d) When we try to minimize the loss function, some of the variables are treated as constants.  Which ones?\n",
    "\n",
    "(e) What matrix being invertible enables us to find the critical point?  (Don't say $X$.  The matrix $X$ is usually not a square matrix, so it can't be invertible.) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f313663a",
   "metadata": {},
   "source": [
    "### Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0933c16a",
   "metadata": {},
   "source": [
    "a) It means the vertical distance between the pridiction y value and the real y value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027b8e2e",
   "metadata": {},
   "source": [
    "b) We are calculating the mean squred error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6cb918",
   "metadata": {},
   "source": [
    "c) No, there will be no way to maximizing the loss function, since the you can always make the current model worse by changing the coefficient.There will always be a worse model, but there will never be a worst model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deda7975",
   "metadata": {},
   "source": [
    "d)Y, X, and m are treated as constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e7f941",
   "metadata": {},
   "source": [
    "e)If the matrix X transpose multiply X is invertible, then we can find the critial point. NO matter X is squre or not, X transpose multiply X is always a squre matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd5765",
   "metadata": {},
   "source": [
    "<p style=\"font-size:20px; color:blue; font-weight:bold\">Question 2:</p>\n",
    "\n",
    "You will need to use some NumPy linear algebra commands, for matrix multiplication, transpose, and inverse.  (Warning: you can't multiply matrices in NumPy using `*`.)\n",
    "\n",
    "(a) Define m to be `10**5` and n to be `10**2`.  Make a matrix X as above of random real numbers uniformly distributed between 0 and 200.  Make a vector theta of length n+1 consisting of random integers between -20 and 20.  Define y to be $X \\vec{\\theta}$.  (So in this case the data has a perfect linear relationship, and we should be able to find theta exactly, up to some numerical precision errors.)\n",
    "\n",
    "(b) Check if the matrix $X^T X$ is invertible.\n",
    "\n",
    "(c) If it is invertible, check if $(X^T X)^{-1} X^T \\vec{y}$ is equal to $\\vec{\\theta}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f77502f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "m = 10**5\n",
    "n = 10**2\n",
    "rng = np.random.default_rng()\n",
    "X = rng.random(size = (m,n))*200\n",
    "new_column = np.ones((m,1))\n",
    "X = np.append(new_column,X, axis=1)\n",
    "theta = rng.random(size = (1,n+1))*40 - 20\n",
    "Y = np.dot(X,np.transpose(theta))\n",
    "try: \n",
    "    np.linalg.inv(np.dot(np.transpose(X),X))\n",
    "except:\n",
    "    print('The matrix is not invertible')\n",
    "\n",
    "result = np.transpose(np.dot(np.dot(np.linalg.inv(np.dot(np.transpose(X),X)),np.transpose(X)),Y))\n",
    "result_list = [result[0,i] for i in range(n+1)]\n",
    "theta_list = [theta[0,i] for i in range(n+1)]\n",
    "print(sum([1 if abs(result_list[i] - theta_list[i]) <= 0.01 else 0 for i in range(n+1)]) == n+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9067ca98",
   "metadata": {},
   "source": [
    "<p style=\"font-size:20px; color:blue; font-weight:bold\">Question 3:</p>\n",
    "\n",
    "How long does it take to compute $(X^T X)^{-1} X^T \\vec{y}$?  What about if you multiply $m$ by 3 (but leave $n$ the same)?  What about if you multiply $n$ by 3 (but leave $m$ the same)?  Which scaling by 3 makes the bigger impact?  Do you know what part of the computation is the most difficult for the computer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a5b29cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.6 ms ± 1.61 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "173 ms ± 15.5 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "413 ms ± 50.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.dot(np.dot(np.linalg.inv(np.dot(np.transpose(X),X)),np.transpose(X)),Y)\n",
    "m = 10**5*3\n",
    "n = 10**2\n",
    "rng = np.random.default_rng()\n",
    "X = rng.random(size = (m,n))*200\n",
    "new_column = np.ones((m,1))\n",
    "X = np.append(new_column,X, axis=1)\n",
    "theta = rng.random(size = (1,n+1))*40 - 20\n",
    "Y = np.dot(X,np.transpose(theta))\n",
    "%timeit np.dot(np.dot(np.linalg.inv(np.dot(np.transpose(X),X)),np.transpose(X)),Y)\n",
    "m = 10**5\n",
    "n = 10**2*3\n",
    "rng = np.random.default_rng()\n",
    "X = rng.random(size = (m,n))*200\n",
    "new_column = np.ones((m,1))\n",
    "X = np.append(new_column,X, axis=1)\n",
    "theta = rng.random(size = (1,n+1))*40 - 20\n",
    "Y = np.dot(X,np.transpose(theta))\n",
    "%timeit np.dot(np.dot(np.linalg.inv(np.dot(np.transpose(X),X)),np.transpose(X)),Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7fea85",
   "metadata": {},
   "source": [
    "n scaling by 3 makes the bigger impact. The hardest part to compute for the computer is the dot product of matrixs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ebbe25",
   "metadata": {},
   "source": [
    "<p style=\"font-size:20px; color:blue; font-weight:bold\">Question 4:</p>\n",
    "\n",
    "Use LinearRegression from `sklearn.linear_model` to solve for theta in another way.  (You can either use `fit_intercept = False`, or you can remove the column of 1s from X.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d391fe9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(X[:,1:], Y)\n",
    "coef_list = [reg.coef_[0,i] for i in range(n)]\n",
    "print(sum([1 if abs(coef_list[i] - theta_list[i+1]) <= 0.01 else 0 for i in range(n)]) == n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
